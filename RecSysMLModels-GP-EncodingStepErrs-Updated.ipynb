{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d70e13",
   "metadata": {},
   "source": [
    "#### We'll will compare the following models for their accuracy and precision. \n",
    "\n",
    "1. Collaborative Filtering (Item-Based)\n",
    "2. Markov Chains\n",
    "3. Random Forest\n",
    "4. Gradient Boosting Machine\n",
    "5. Recurrent Neural Networks (RNN) - LSTM\n",
    "\n",
    "To get us started we will set up the preprocessing and helper functions that will be used by all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd41089e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 08:06:26.303246: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-28 08:06:27.637867: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from dask import delayed, compute\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c660807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for checkpoints\n",
    "train_data_path = 'train_data.pkl'\n",
    "test_data_path = 'test_data.pkl'\n",
    "processed_data_path = 'processed_data.pkl'\n",
    "models_predictions_path = 'models_predictions.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7745af7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save data\n",
    "def save_data(data, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "# Function to load data\n",
    "def load_data(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9dc5b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded preprocessed data from checkpoint.\n"
     ]
    }
   ],
   "source": [
    "# Check if preprocessed data exists\n",
    "try:\n",
    "    (train_data, test_data, train_sequences, test_sequences, encoder, \n",
    "     encoded_test_sequences, encoded_train_sequences, X_train, y_train, \n",
    "     X_test, y_test) = load_data(processed_data_path)\n",
    "    print(\"Loaded preprocessed data from checkpoint.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Preprocessed data not found. Running preprocessing steps...\")\n",
    "\n",
    "    # Load and sample datasets using Dask\n",
    "    train_data = dd.read_csv('train_set.csv').sample(frac=0.01)\n",
    "    test_data = dd.read_csv('test_set.csv').sample(frac=0.01)\n",
    "\n",
    "    # Convert and combine features\n",
    "    for data in [train_data, test_data]:\n",
    "        data['checkin'] = data['checkin'].astype(str)\n",
    "        data['utrip_id_checkin'] = data['utrip_id'].astype(str) + '_' + data['checkin']\n",
    "        data['city_country'] = data['city_id'].astype(str) + '_' + data['hotel_country'].astype(str)\n",
    "        data['city_country'] = data['city_country'].fillna('missing')\n",
    "        data = data.categorize(columns=['city_country'])\n",
    "    \n",
    "    # Create sequences\n",
    "    def create_sequences(data):\n",
    "        return data.groupby('utrip_id')['city_country'].apply(list).compute().tolist()\n",
    "    \n",
    "    with ProgressBar():\n",
    "        train_sequences = create_sequences(train_data)\n",
    "        test_sequences = create_sequences(test_data)\n",
    "\n",
    "    # Encode city_country strings as integers\n",
    "    all_sequences = train_sequences + test_sequences\n",
    "    all_cities_countries = list(set(city_country for seq in all_sequences for city_country in seq))\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(all_cities_countries)\n",
    "\n",
    "    def encode_sequences(sequences):\n",
    "        return [encoder.transform(seq).tolist() for seq in sequences]\n",
    "\n",
    "    encoded_train_sequences = encode_sequences(train_sequences)\n",
    "    encoded_test_sequences = encode_sequences(test_sequences)\n",
    "\n",
    "    # Prepare data for training models\n",
    "    def prepare_data(sequences, sequence_length=None):\n",
    "        if sequence_length is None:\n",
    "            sequence_length = min(max(len(seq) for seq in sequences), 100)  # Adjust threshold as needed\n",
    "        X, y = [], []\n",
    "        for seq in tqdm(sequences, desc=\"Preparing data\"):\n",
    "            for i in range(1, len(seq)):\n",
    "                X.append(seq[:i])\n",
    "                y.append(seq[i])\n",
    "        X = pad_sequences(X, maxlen=sequence_length, padding='pre')\n",
    "        y = np.array(y)\n",
    "        return X, y\n",
    "\n",
    "    X_train, y_train = prepare_data(encoded_train_sequences)\n",
    "    X_test, y_test = prepare_data(encoded_test_sequences, sequence_length=X_train.shape[1])\n",
    "\n",
    "    # Save preprocessed data to checkpoint\n",
    "    save_data((train_data.compute(), test_data.compute(), train_sequences, test_sequences, encoder, \n",
    "               encoded_test_sequences, encoded_train_sequences, X_train, y_train, X_test, y_test), \n",
    "              processed_data_path)\n",
    "    print(\"Saved preprocessed data to checkpoint.\")\n",
    "\n",
    "    # Print shapes to verify the data preparation\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57608e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of X_train:\n",
      "[[   0    0 4016]\n",
      " [   0    0  288]\n",
      " [   0    0 2432]\n",
      " [   0    0 4721]\n",
      " [   0    0 4336]]\n",
      "\n",
      "First 5 elements of y_train:\n",
      "[2589 4557 2835 2731 1956]\n",
      "\n",
      "First 5 rows of X_train (as DataFrame):\n",
      "   0  1     2\n",
      "0  0  0  4016\n",
      "1  0  0   288\n",
      "2  0  0  2432\n",
      "3  0  0  4721\n",
      "4  0  0  4336\n",
      "\n",
      "First 5 elements of y_train (as DataFrame):\n",
      "0    2589\n",
      "1    4557\n",
      "2    2835\n",
      "3    2731\n",
      "4    1956\n",
      "Name: Target, dtype: int64\n",
      "\n",
      "First 5 rows of X_test (as DataFrame):\n",
      "   0  1     2\n",
      "0  0  0     0\n",
      "1  0  0  1569\n",
      "2  0  0  1229\n",
      "3  0  0  2179\n",
      "4  0  0  3246\n",
      "\n",
      "First 5 elements of y_test (as DataFrame):\n",
      "0    2415\n",
      "1    1799\n",
      "2    3274\n",
      "3    4661\n",
      "4    4588\n",
      "Name: Target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 rows of X_train and y_train\n",
    "print(\"First 5 rows of X_train:\")\n",
    "print(X_train[:5])\n",
    "\n",
    "print(\"\\nFirst 5 elements of y_train:\")\n",
    "print(y_train[:5])\n",
    "\n",
    "# Convert to Dask DataFrame for better readability\n",
    "X_train_dd = dd.from_pandas(pd.DataFrame(X_train), npartitions=5)\n",
    "y_train_dd = dd.from_pandas(pd.Series(y_train, name='Target'), npartitions=5)\n",
    "\n",
    "X_test_dd = dd.from_pandas(pd.DataFrame(X_test), npartitions=5)\n",
    "y_test_dd = dd.from_pandas(pd.Series(y_test, name='Target'), npartitions=5)\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(\"\\nFirst 5 rows of X_train (as DataFrame):\")\n",
    "print(X_train_dd.head())\n",
    "\n",
    "print(\"\\nFirst 5 elements of y_train (as DataFrame):\")\n",
    "print(y_train_dd.head())\n",
    "\n",
    "print(\"\\nFirst 5 rows of X_test (as DataFrame):\")\n",
    "print(X_test_dd.head())\n",
    "\n",
    "print(\"\\nFirst 5 elements of y_test (as DataFrame):\")\n",
    "print(y_test_dd.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b03823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_city_country = set(city_country for seq in all_sequences for city_country in seq)\n",
    "# print(\"Unique city_country values:\", len(unique_city_country))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dd1082",
   "metadata": {},
   "source": [
    "### Collaborative Filtering (Item-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00dddb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collaborative Filtering Complete\n",
      "First 5 collaborative filtering predictions:\n",
      "['30129_Bultan', '30129_Bultan', '30129_Bultan', '30129_Bultan', '30129_Bultan']\n"
     ]
    }
   ],
   "source": [
    "# Collaborative Filtering (Item-Based)\n",
    "# Create a co-occurrence matrix\n",
    "item_cooccurrence_matrix = np.zeros((len(encoder.classes_), len(encoder.classes_)))\n",
    "\n",
    "for seq in encoded_train_sequences:\n",
    "    for i in range(len(seq)):\n",
    "        for j in range(i + 1, len(seq)):\n",
    "            item_cooccurrence_matrix[seq[i], seq[j]] += 1\n",
    "            item_cooccurrence_matrix[seq[j], seq[i]] += 1\n",
    "\n",
    "# Use TruncatedSVD for dimensionality reduction\n",
    "embedding_dim = 50\n",
    "svd = TruncatedSVD(n_components=embedding_dim)\n",
    "item_embeddings = svd.fit_transform(item_cooccurrence_matrix)\n",
    "\n",
    "# Calculate cosine similarity matrix for embeddings\n",
    "item_sim_matrix = cosine_similarity(item_embeddings)\n",
    "\n",
    "def collaborative_filtering_predict(current_place):\n",
    "    if current_place in encoder.classes_:\n",
    "        current_idx = encoder.transform([current_place])[0]\n",
    "        similarity_scores = item_sim_matrix[current_idx]\n",
    "        most_similar_idx = similarity_scores.argsort()[-2]\n",
    "        return encoder.inverse_transform([most_similar_idx])[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Collaborative Filtering Predictions\n",
    "collab_preds = [collaborative_filtering_predict(encoder.inverse_transform([seq[-1]])[0]) for seq in encoded_test_sequences]\n",
    "\n",
    "print(\"Collaborative Filtering Complete\")\n",
    "# Print the first 5 collaborative filtering predictions\n",
    "\n",
    "print(\"First 5 collaborative filtering predictions:\")\n",
    "print(collab_preds[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2d20d0",
   "metadata": {},
   "source": [
    "### Markov Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3c7026a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markov Complete\n",
      "First 5 Markov chain predictions:\n",
      "['unknown', 'unknown', 'unknown', 'unknown', 'unknown']\n"
     ]
    }
   ],
   "source": [
    "# Markov Chains\n",
    "# Create transition pairs from the city_country chains\n",
    "transitions = []\n",
    "\n",
    "for chain in encoded_train_sequences:\n",
    "    for i in range(len(chain) - 1):\n",
    "        transitions.append((chain[i], chain[i + 1]))\n",
    "\n",
    "# Create a DataFrame for transitions\n",
    "transitions_df = pd.DataFrame(transitions, columns=['current_place', 'next_place'])\n",
    "\n",
    "# Calculate transition probabilities\n",
    "transition_counts = transitions_df.groupby('current_place')['next_place'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "\n",
    "# Function to predict the next place based on the current place\n",
    "def markov_chain_predict(current_place):\n",
    "    if current_place in transition_counts.index:\n",
    "        return transition_counts.loc[current_place].idxmax()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Generate predictions using the Markov chain model\n",
    "markov_preds = [markov_chain_predict(seq[-1]) for seq in encoded_test_sequences]\n",
    "\n",
    "# Convert predictions back to original city_country labels\n",
    "markov_preds = [encoder.inverse_transform([pred])[0] if pred is not None else 'unknown' for pred in markov_preds]\n",
    "\n",
    "print(\"Markov Complete\")\n",
    "\n",
    "# Print the first 5 Markov chain predictions\n",
    "print(\"First 5 Markov chain predictions:\")\n",
    "print(markov_preds[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c9f1056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to ensure that y_test and predictions have consistent lengths\n",
    "def filter_valid_predictions(y_true, y_pred):\n",
    "    y_true_filtered, y_pred_filtered = [], []\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if pred is not None:  # Filter out 'None' predictions\n",
    "            y_true_filtered.append(true)\n",
    "            y_pred_filtered.append(pred)\n",
    "    return y_true_filtered, y_pred_filtered\n",
    "\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    # Ensure y_pred contains only labels present in encoder.classes_\n",
    "    y_pred_mapped = []\n",
    "    for label in y_pred:\n",
    "        if label in encoder.classes_:\n",
    "            y_pred_mapped.append(label)\n",
    "        else:\n",
    "            # Handle previously unseen labels, e.g., by mapping to a default label\n",
    "            y_pred_mapped.append('unknown')  # Replace with appropriate handling\n",
    "\n",
    "    # Filter y_true and y_pred to only include valid pairs\n",
    "    y_true_filtered, y_pred_filtered = filter_valid_predictions(y_true, y_pred_mapped)\n",
    "\n",
    "    # Transform y_true and y_pred_filtered\n",
    "    y_true_encoded = encoder.transform(y_true_filtered)\n",
    "    y_pred_encoded = encoder.transform(y_pred_filtered)\n",
    "\n",
    "    accuracy = accuracy_score(y_true_encoded, y_pred_encoded)\n",
    "    precision = precision_score(y_true_encoded, y_pred_encoded, average='weighted', zero_division=1)\n",
    "    return accuracy, precision\n",
    "\n",
    "# Random Forest Model\n",
    "@delayed\n",
    "def train_rf(X_train, y_train, X_test):\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_preds = rf_model.predict(X_test)\n",
    "    rf_preds = encoder.inverse_transform(rf_preds)\n",
    "    return rf_preds\n",
    "\n",
    "# Gradient Boosting Model\n",
    "@delayed\n",
    "def train_gbm(X_train, y_train, X_test):\n",
    "    gbm_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "    gbm_model.fit(X_train, y_train)\n",
    "    gbm_preds = gbm_model.predict(X_test)\n",
    "    gbm_preds = encoder.inverse_transform(gbm_preds)\n",
    "    return gbm_preds\n",
    "\n",
    "# LSTM Model\n",
    "def train_lstm(X_train, y_train, X_test):\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(Embedding(input_dim=len(encoder.classes_), output_dim=50, input_length=X_train.shape[1]))\n",
    "    lstm_model.add(LSTM(100, return_sequences=False))\n",
    "    lstm_model.add(Dense(len(encoder.classes_), activation='softmax'))\n",
    "    lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    lstm_model.fit(X_train, y_train, epochs=50, batch_size=64, validation_split=0.2)\n",
    "\n",
    "    # Predict probabilities using softmax output\n",
    "    lstm_preds = lstm_model.predict(X_test)\n",
    "\n",
    "    # Use np.argmax to get the index of the class with the highest probability\n",
    "    lstm_preds_idx = np.argmax(lstm_preds, axis=1)\n",
    "\n",
    "    # Convert the predicted indices back to original labels\n",
    "    lstm_preds_labels = encoder.inverse_transform(lstm_preds_idx)\n",
    "\n",
    "    return lstm_preds_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b1ad8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajay/Documents/projects/RecSys-local/.venv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2024-05-28 08:06:54.006405: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-28 08:06:54.199305: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.0028 - loss: 8.4727 - val_accuracy: 0.0000e+00 - val_loss: 8.4716\n",
      "Epoch 2/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0211 - loss: 8.4650 - val_accuracy: 0.0000e+00 - val_loss: 8.4698\n",
      "Epoch 3/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0103 - loss: 8.4541 - val_accuracy: 0.0175 - val_loss: 8.4666\n",
      "Epoch 4/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0193 - loss: 8.4339 - val_accuracy: 0.0175 - val_loss: 8.4596\n",
      "Epoch 5/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0146 - loss: 8.3898 - val_accuracy: 0.0175 - val_loss: 8.4436\n",
      "Epoch 6/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0089 - loss: 8.2837 - val_accuracy: 0.0175 - val_loss: 8.4071\n",
      "Epoch 7/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0162 - loss: 8.0411 - val_accuracy: 0.0175 - val_loss: 8.3373\n",
      "Epoch 8/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0120 - loss: 7.5431 - val_accuracy: 0.0175 - val_loss: 8.2603\n",
      "Epoch 9/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0131 - loss: 6.7394 - val_accuracy: 0.0175 - val_loss: 8.3631\n",
      "Epoch 10/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0079 - loss: 5.9426 - val_accuracy: 0.0175 - val_loss: 8.8542\n",
      "Epoch 11/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0063 - loss: 5.5118 - val_accuracy: 0.0175 - val_loss: 9.4996\n",
      "Epoch 12/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0105 - loss: 5.3510 - val_accuracy: 0.0175 - val_loss: 10.0273\n",
      "Epoch 13/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0131 - loss: 5.2792 - val_accuracy: 0.0175 - val_loss: 10.4091\n",
      "Epoch 14/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0190 - loss: 5.2824 - val_accuracy: 0.0175 - val_loss: 10.6803\n",
      "Epoch 15/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0174 - loss: 5.2446 - val_accuracy: 0.0175 - val_loss: 10.8787\n",
      "Epoch 16/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0268 - loss: 5.2443 - val_accuracy: 0.0175 - val_loss: 11.0256\n",
      "Epoch 17/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0195 - loss: 5.2559 - val_accuracy: 0.0175 - val_loss: 11.1333\n",
      "Epoch 18/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0211 - loss: 5.2510 - val_accuracy: 0.0175 - val_loss: 11.2141\n",
      "Epoch 19/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0127 - loss: 5.2525 - val_accuracy: 0.0175 - val_loss: 11.2769\n",
      "Epoch 20/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0143 - loss: 5.2607 - val_accuracy: 0.0175 - val_loss: 11.3285\n",
      "Epoch 21/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0028 - loss: 5.2433 - val_accuracy: 0.0175 - val_loss: 11.3725\n",
      "Epoch 22/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0206 - loss: 5.2670 - val_accuracy: 0.0175 - val_loss: 11.4094\n",
      "Epoch 23/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0206 - loss: 5.2257 - val_accuracy: 0.0175 - val_loss: 11.4421\n",
      "Epoch 24/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0138 - loss: 5.2394 - val_accuracy: 0.0175 - val_loss: 11.4758\n",
      "Epoch 25/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0190 - loss: 5.2571 - val_accuracy: 0.0175 - val_loss: 11.5032\n",
      "Epoch 26/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0148 - loss: 5.2521 - val_accuracy: 0.0175 - val_loss: 11.5296\n",
      "Epoch 27/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0206 - loss: 5.2484 - val_accuracy: 0.0175 - val_loss: 11.5587\n",
      "Epoch 28/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0101 - loss: 5.2508 - val_accuracy: 0.0175 - val_loss: 11.5870\n",
      "Epoch 29/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0138 - loss: 5.2168 - val_accuracy: 0.0175 - val_loss: 11.6098\n",
      "Epoch 30/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0195 - loss: 5.2468 - val_accuracy: 0.0175 - val_loss: 11.6323\n",
      "Epoch 31/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0252 - loss: 5.2377 - val_accuracy: 0.0175 - val_loss: 11.6528\n",
      "Epoch 32/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.0148 - loss: 5.2475 - val_accuracy: 0.0175 - val_loss: 11.6758\n",
      "Epoch 33/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0164 - loss: 5.2335 - val_accuracy: 0.0175 - val_loss: 11.6978\n",
      "Epoch 34/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0164 - loss: 5.2671 - val_accuracy: 0.0175 - val_loss: 11.7212\n",
      "Epoch 35/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0164 - loss: 5.2564 - val_accuracy: 0.0175 - val_loss: 11.7422\n",
      "Epoch 36/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0159 - loss: 5.2387 - val_accuracy: 0.0175 - val_loss: 11.7657\n",
      "Epoch 37/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0190 - loss: 5.2636 - val_accuracy: 0.0175 - val_loss: 11.7860\n",
      "Epoch 38/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0164 - loss: 5.2576 - val_accuracy: 0.0175 - val_loss: 11.8090\n",
      "Epoch 39/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0148 - loss: 5.2484 - val_accuracy: 0.0175 - val_loss: 11.8309\n",
      "Epoch 40/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0107 - loss: 5.2575 - val_accuracy: 0.0175 - val_loss: 11.8505\n",
      "Epoch 41/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0127 - loss: 5.2579 - val_accuracy: 0.0175 - val_loss: 11.8673\n",
      "Epoch 42/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0206 - loss: 5.2452 - val_accuracy: 0.0175 - val_loss: 11.8831\n",
      "Epoch 43/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0143 - loss: 5.2495 - val_accuracy: 0.0175 - val_loss: 11.8984\n",
      "Epoch 44/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0211 - loss: 5.2277 - val_accuracy: 0.0175 - val_loss: 11.9158\n",
      "Epoch 45/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0237 - loss: 5.2534 - val_accuracy: 0.0175 - val_loss: 11.9338\n",
      "Epoch 46/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0159 - loss: 5.2370 - val_accuracy: 0.0175 - val_loss: 11.9506\n",
      "Epoch 47/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.0252 - loss: 5.2268 - val_accuracy: 0.0175 - val_loss: 11.9716\n",
      "Epoch 48/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0148 - loss: 5.2266 - val_accuracy: 0.0175 - val_loss: 11.9916\n",
      "Epoch 49/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0127 - loss: 5.2572 - val_accuracy: 0.0175 - val_loss: 12.0121\n",
      "Epoch 50/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0206 - loss: 5.2368 - val_accuracy: 0.0175 - val_loss: 12.0309\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "Saved model predictions to checkpoint.\n"
     ]
    }
   ],
   "source": [
    "# Train and predict using Dask delayed functions\n",
    "rf_preds, gbm_preds, lstm_preds = compute(train_rf(X_train, y_train, X_test), train_gbm(X_train, y_train, X_test), train_lstm(X_train, y_train, X_test))\n",
    "\n",
    "# Save model predictions to checkpoint\n",
    "save_data((collab_preds, markov_preds, rf_preds, gbm_preds, lstm_preds), models_predictions_path)\n",
    "print(\"Saved model predictions to checkpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4af04408",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: '2415'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/projects/RecSys-local/.venv/lib/python3.10/site-packages/sklearn/utils/_encode.py:225\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/projects/RecSys-local/.venv/lib/python3.10/site-packages/sklearn/utils/_encode.py:165\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[0;32m~/Documents/projects/RecSys-local/.venv/lib/python3.10/site-packages/sklearn/utils/_encode.py:165\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[0;32m~/Documents/projects/RecSys-local/.venv/lib/python3.10/site-packages/sklearn/utils/_encode.py:159\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: '2415'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluating Predictions\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m collab_accuracy, collab_precision \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollab_preds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# markov_accuracy, markov_precision = evaluate_model(y_test, markov_preds)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m rf_accuracy, rf_precision \u001b[38;5;241m=\u001b[39m evaluate_model(encoder\u001b[38;5;241m.\u001b[39minverse_transform(y_test), rf_preds)\n",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     23\u001b[0m y_true_filtered, y_pred_filtered \u001b[38;5;241m=\u001b[39m filter_valid_predictions(y_true, y_pred_mapped)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Transform y_true and y_pred_filtered\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m y_true_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true_filtered\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m y_pred_encoded \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mtransform(y_pred_filtered)\n\u001b[1;32m     29\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_true_encoded, y_pred_encoded)\n",
      "File \u001b[0;32m~/Documents/projects/RecSys-local/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:137\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/RecSys-local/.venv/lib/python3.10/site-packages/sklearn/utils/_encode.py:227\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 227\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: '2415'"
     ]
    }
   ],
   "source": [
    "# Evaluating Predictions\n",
    "collab_accuracy, collab_precision = evaluate_model(y_test, collab_preds)\n",
    "# markov_accuracy, markov_precision = evaluate_model(y_test, markov_preds)\n",
    "rf_accuracy, rf_precision = evaluate_model(encoder.inverse_transform(y_test), rf_preds)\n",
    "gbm_accuracy, gbm_precision = evaluate_model(encoder.inverse_transform(y_test), gbm_preds)\n",
    "lstm_accuracy, lstm_precision = evaluate_model(encoder.inverse_transform(y_test), lstm_preds)\n",
    "\n",
    "# Print the results\n",
    "# print(f\"Collaborative Filtering - Accuracy: {collab_accuracy:.2f}, Precision: {collab_precision:.2f}\")\n",
    "# print(f\"Markov Chains - Accuracy: {markov_accuracy:.2f}, Precision: {markov_precision:.2f}\")\n",
    "print(f\"Random Forest - Accuracy: {rf_accuracy:.2f}, Precision: {rf_precision:.2f}\")\n",
    "print(f\"Gradient Boosting - Accuracy: {gbm_accuracy:.2f}, Precision: {gbm_precision:.2f}\")\n",
    "print(f\"LSTM - Accuracy: {lstm_accuracy:.2f}, Precision: {lstm_precision:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f7cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save predictions to CSV\n",
    "def save_predictions(predictions, filename, current_city):\n",
    "    preds_df = pd.DataFrame(predictions, columns=['predicted_next_city_country'])\n",
    "    preds_df['current_city_country'] = current_city\n",
    "    preds_df.to_csv(filename, index=False)\n",
    "    print(f'Predictions written to {filename}')\n",
    "\n",
    "# Prepare current city data for reference\n",
    "current_city = [encoder.inverse_transform([seq[-1]])[0] for seq in encoded_test_sequences]\n",
    "\n",
    "# Save the predictions for each model\n",
    "model_predictions = {\n",
    "    'collab_predictions.csv': collab_preds,\n",
    "    'markov_predictions.csv': markov_preds,\n",
    "    'rf_predictions.csv': rf_preds,\n",
    "    'gbm_predictions.csv': gbm_preds,\n",
    "    'lstm_predictions.csv': lstm_preds\n",
    "}\n",
    "\n",
    "for filename, preds in model_predictions.items():\n",
    "    save_predictions(preds, filename, current_city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f98b20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
